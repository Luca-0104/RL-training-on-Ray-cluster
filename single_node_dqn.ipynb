{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bfe8a64-2856-48ee-b5ad-4b8603adef37",
   "metadata": {},
   "source": [
    "# Train a DQN model on a single EC2 instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41096af6-1c65-40d4-9e22-73b4f65166ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code is for demonstration purpose. \n",
    "You will not connect to the ray cluster.\n",
    "You will train a DQN model on a single node.\n",
    "'''\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0553fe3e-c546-4e5c-96e4-21719948a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DO NOT MODIFY THIS CELL.\n",
    "We use DQN as our RL training algorithm\n",
    "See https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm to get more infomation about DQN.\n",
    "'''\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_unit=64,\n",
    "                 fc2_unit=64):\n",
    "        \"\"\"\n",
    "        Initialize parameters and build model.\n",
    "        Params\n",
    "        =======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_unit (int): Number of nodes in first hidden layer\n",
    "            fc2_unit (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_unit)\n",
    "        self.fc2 = nn.Linear(fc1_unit, fc2_unit)\n",
    "        self.fc3 = nn.Linear(fc2_unit, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Build a network that maps state -> action values.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe2793-e206-4e89-97d5-767feb42298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DO NOT MODIFY THIS CELL.\n",
    "The ReplayBuffer is an essential component in Deep Q-Networks (DQN)\n",
    "It stores the Agent's experiences at each time step, comprised of the state, action, reward, next state, and done flag, in a memory buffer.\n",
    "This allows for the experiences to be randomly sampled later to train the neural network.\n",
    "'''\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float()\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float()\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0337df-4113-4378-9e48-16c2ce122513",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The Agent is interacting with the environment to learn an optimal policy for decision-making.\n",
    "'''\n",
    "class Agent:\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "        # Set test enviroment\n",
    "        self.test_env= gym.make('LunarLander-v2')\n",
    "        self.test_env.seed(TEST_SEED)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states\n",
    "        q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update target network\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    # test agent by playing games and return average rewards\n",
    "    def play_game(self, max_timesteps, num_games=10):\n",
    "        total_reward_sum = 0\n",
    "        for _ in range(num_games):\n",
    "            state = self.test_env.reset()\n",
    "            total_reward = 0\n",
    "            for t in range(max_timesteps):\n",
    "\n",
    "                state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    action_values = self.qnetwork_local(state_tensor)\n",
    "                action = torch.argmax(action_values).item()\n",
    "\n",
    "                next_state, reward, done, _ = self.test_env.step(action)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "            total_reward_sum += total_reward\n",
    "\n",
    "        average_reward = total_reward_sum / num_games\n",
    "        return average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf595b4-77ec-48c0-9ff3-b8d84d77b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code iteratively trains an Agent over a specified number of episodes,\n",
    "adjusting the exploration-exploitation balance via the epsilon decay.\n",
    "It tracks the Agent's performance, evaluating and reporting the average score periodically.\n",
    "Training concludes either when the maximum number of episodes is reached or the agent consistently achieves a predefined average score,\n",
    "suggesting the environment is solved.\n",
    "'''\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "\n",
    "    eps = eps_start  # initialize epsilon\n",
    "    average_score=0\n",
    "    # train the model\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        time_start=time.time()\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        eps = max(eps_end, eps_decay * eps)  # decrease epsilon\n",
    "        time_end=time.time()\n",
    "        time_train.append(time_end-time_start)\n",
    "        print(f'Episode {i_episode} finished.')\n",
    "        # test every 10 episodes or the average score of testing is near 100\n",
    "        if i_episode % 10 == 0 or average_score>=90:\n",
    "            average_score = agent.play_game(200, num_games=10)\n",
    "            print(f'Episode {i_episode}\\tAverage Score: {average_score:.2f}')\n",
    "\n",
    "        # end training if avg score >=100\n",
    "        if average_score >= 100:\n",
    "            print(f'Environment solved in {i_episode} episodes!')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32663dd4-0091-4ac4-9c96-e2cb7b71f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The driver code to run the training. \n",
    "'''\n",
    "\n",
    "# set random seed; DO NOT CHANGE\n",
    "SEED=0  # seed for train\n",
    "TEST_SEED=42 # seed for test\n",
    "\n",
    "# hyper parameters\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64  # minibatch size\n",
    "GAMMA = 0.99  # discount factor\n",
    "TAU = 1e-3  # for soft update of target parameters\n",
    "LR = 5e-4  # learning rate\n",
    "UPDATE_EVERY = 4  # how often to update the network\n",
    "\n",
    "\n",
    "# initialize environment for training\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(SEED)\n",
    "\n",
    "# initialize agent\n",
    "agent = Agent(state_size=8, action_size=4, seed=SEED)\n",
    "\n",
    "# record training time\n",
    "time_train=[]\n",
    "start_time = time.time()\n",
    "\n",
    "# run the training session\n",
    "dqn()\n",
    "end_ime= time.time()\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"Total time: {total_time:.2f} seconds\")\n",
    "print(f\"Total Training Time: {sum(time_train)} seconds\")\n",
    "print(f\"Average time per episode: {np.mean(time_train)} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
