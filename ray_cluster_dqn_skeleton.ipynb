{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbac0471-5025-42e4-8a04-a8d078df52ef",
   "metadata": {},
   "source": [
    "# Part 3: Train a DQN on a Ray cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bdeffdf-27fd-44fa-8a08-439993f18390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Ray Client is already connected. Maybe you called ray.init(\"ray://<address>\") twice by accident?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# In this part, you will connect to your Ray cluster.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# TODO: change the IP address of your head node.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mray://172.31.22.219:10001\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/_private/worker.py:1430\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, labels, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m passed_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1429\u001b[0m builder\u001b[38;5;241m.\u001b[39m_init_args(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpassed_kwargs)\n\u001b[0;32m-> 1430\u001b[0m ctx \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01musage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m usage_lib\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m passed_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_multiple\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/client_builder.py:173\u001b[0m, in \u001b[0;36mClientBuilder.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_multiple_connections:\n\u001b[1;32m    171\u001b[0m     old_ray_cxt \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mray\u001b[38;5;241m.\u001b[39mset_context(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 173\u001b[0m client_info_dict \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_connect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_job_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_credentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_init_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remote_init_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m dashboard_url \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mray\u001b[38;5;241m.\u001b[39m_get_dashboard_url()\n\u001b[1;32m    183\u001b[0m cxt \u001b[38;5;241m=\u001b[39m ClientContext(\n\u001b[1;32m    184\u001b[0m     dashboard_url\u001b[38;5;241m=\u001b[39mdashboard_url,\n\u001b[1;32m    185\u001b[0m     python_version\u001b[38;5;241m=\u001b[39mclient_info_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython_version\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     _context_to_restore\u001b[38;5;241m=\u001b[39mray\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mray\u001b[38;5;241m.\u001b[39mget_context(),\n\u001b[1;32m    191\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/util/client_connect.py:42\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(conn_str, secure, metadata, connection_retries, job_config, namespace, ignore_version, _credentials, ray_init_kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling ray.init() again after it has already been called. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReusing the existing Ray client connection.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         )\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ray\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mclient_worker\u001b[38;5;241m.\u001b[39mconnection_info()\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRay Client is already connected. Maybe you called \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mray.init(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mray://<address>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) twice by accident?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Enable the same hooks that RAY_CLIENT_MODE does, as calling\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# ray.init(\"ray://<address>\") is specifically for using client mode.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m _set_client_hook_status(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Ray Client is already connected. Maybe you called ray.init(\"ray://<address>\") twice by accident?"
     ]
    }
   ],
   "source": [
    "# In this part, you will connect to your Ray cluster.\n",
    "# TODO: change the IP address of your head node.\n",
    "\n",
    "import ray\n",
    "ray.init(address=\"ray://172.31.22.219:10001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb7a61ea-3b0b-459d-92d6-33bfaea545a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This cluster consists of\n",
      "    5 nodes in total\n",
      "    10.0 CPU resources in total\n",
      "    25.15036735869944 Memory resources in total\n",
      "    11.089107510633767 Object store memory resources in total\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Print the status of the Ray cluster.\n",
    "print('''This cluster consists of\n",
    "    {} nodes in total\n",
    "    {} CPU resources in total\n",
    "    {} Memory resources in total\n",
    "    {} Object store memory resources in total\n",
    "    '''.format(len(ray.nodes()), ray.cluster_resources()['CPU'], \n",
    "           ray.cluster_resources()['memory'] / (1024*1024*1024), \n",
    "           ray.cluster_resources()['object_store_memory'] / (1024*1024*1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a68ebdf9-fd8d-4c33-975e-2615832a67d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks executed\n",
      "    775 tasks on 172.31.22.219 with Gym version 0.21.0\n",
      "    199 tasks on 172.31.29.149 with Gym version 0.21.0\n",
      "    22 tasks on 172.31.17.183 with Gym version 0.21.0\n",
      "    2 tasks on 172.31.22.79 with Gym version 0.21.0\n",
      "    2 tasks on 172.31.30.214 with Gym version 0.21.0\n",
      "CPU times: user 991 ms, sys: 162 ms, total: 1.15 s\n",
      "Wall time: 3.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "(Optional testing code) \n",
    "Test your Ray cluster, see the outputs to check if all nodes are ready to work\n",
    "the output should be like \"123 tasks on 1.2.3.4 with Gym version 0.21.0\".\n",
    "By default, you have 5 nodes, one is the head node, the others are workers (actors).\n",
    "'''\n",
    "import socket\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "@ray.remote\n",
    "def f():\n",
    "    import torch\n",
    "    import gym\n",
    "    time.sleep(0.001)\n",
    "    return socket.gethostbyname(socket.gethostname()),gym.__version__\n",
    "\n",
    "object_ids = [f.remote() for _ in range(1000)]\n",
    "results = ray.get(object_ids)\n",
    "\n",
    "print('Tasks executed')\n",
    "\n",
    "stats = {}\n",
    "\n",
    "for ip_address, version in results:\n",
    "    if ip_address in stats:\n",
    "        stats[ip_address] = (stats[ip_address][0] + 1, version)\n",
    "    else:\n",
    "        stats[ip_address] = (1, version)\n",
    "\n",
    "for ip_address, (num_tasks, version) in stats.items():\n",
    "    print('    {} tasks on {} with Gym version {}'.format(num_tasks, ip_address, version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4948b764-baf9-439b-875e-3990b3a9d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import ray\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95d07be1-fa45-4609-8d81-b74d09a0fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DO NOT MODIFY THIS CELL.\n",
    "We use DQN as our RL training algorithm.\n",
    "See https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm to get more infomation about DQN.\n",
    "'''\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_unit=64,\n",
    "                 fc2_unit=64):\n",
    "        \"\"\"\n",
    "        Initialize parameters and build model.\n",
    "        Params\n",
    "        =======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_unit (int): Number of nodes in first hidden layer\n",
    "            fc2_unit (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_unit)\n",
    "        self.fc2 = nn.Linear(fc1_unit, fc2_unit)\n",
    "        self.fc3 = nn.Linear(fc2_unit, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Build a network that maps state -> action values.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c15577a-aefd-47e6-9c39-d0aa26a2a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DO NOT MODIFY THIS CELL.\n",
    "The ReplayBuffer is an essential component in Deep Q-Networks (DQN)\n",
    "It stores the Agent's experiences at each time step, comprised of the state, action, reward, next state, and done flag, in a memory buffer.\n",
    "This allows for the experiences to be randomly sampled later to train the neural network.\n",
    "'''\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float()\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float()\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f84dbe2-2d48-4264-9340-52e71b297aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DO NOT MODIFTY THIS CELL.\n",
    "The Learner is interacting with the environment to learn an optimal policy for decision-making.\n",
    "The logic is the same as the Agent in the DQN algorithm, with only a different name here.\n",
    "'''\n",
    "class Learner:\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize a Learner object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, gamma)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states\n",
    "        q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update target network\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25f2b382-4b10-45e1-9e6e-bf5914142799",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This class implements a global network (i.e., a parameter server) in a distributed training setup.\n",
    "It holds a global model that aggregates updates from multiple Agent actors to collectively learn a policy.\n",
    "GlobalNet will also periodically test the learned policy in a dedicated test environment.\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "TODO: Task 0\n",
    "\"\"\"\n",
    "# Task 0 TODO: add Ray actor decorator here\n",
    "# Hint: You need to specify the number of CPUs for this actor\n",
    "\n",
    "# TODO: Your code here...\n",
    "\n",
    "\"\"\"\n",
    "End of Task 0\n",
    "\"\"\"\n",
    "@ray.remote(num_cpus=2)\n",
    "class GlobalNet:\n",
    "    def __init__(self, state_size, action_size, seed, fc1_unit=64, fc2_unit=64):\n",
    "        random.seed(seed)\n",
    "        self.local_model = QNetwork(state_size, action_size, seed, fc1_unit, fc2_unit)\n",
    "        self.test_env = gym.make('LunarLander-v2')\n",
    "        self.test_env.seed(TEST_SEED)\n",
    "        print(f\"Initializing an Global Network on worker with IP: {self.get_worker_ip()}\")\n",
    "\n",
    "    def get_worker_ip(self):\n",
    "        \"\"\"Get the current worker's IP address.\"\"\"\n",
    "        hostname = socket.gethostname()\n",
    "        ip_address = socket.gethostbyname(hostname)\n",
    "        return ip_address\n",
    "\n",
    "    def average_weights(self, actors):\n",
    "        \"\"\"\n",
    "        Collects weights from all given actors and averages them to update the global model.\n",
    "\n",
    "        Params:\n",
    "            actors: list of actor handlers from which to collect weights\n",
    "        \"\"\"\n",
    "        weights_futures = [actor.get_weights.remote() for actor in actors]\n",
    "        local_weights_list = ray.get(weights_futures)\n",
    "        local_state_dict = self.local_model.state_dict()\n",
    "\n",
    "        # Initialize average weights\n",
    "        average_local_weights = {k: torch.zeros_like(v) for k, v in local_state_dict.items()}\n",
    "        # Sum all the weights\n",
    "        for local_state in local_weights_list:\n",
    "            for k, v in local_state.items():\n",
    "                average_local_weights[k] += v\n",
    "        # Average the weights\n",
    "        for k in average_local_weights.keys():\n",
    "            average_local_weights[k] = average_local_weights[k] / len(local_weights_list)\n",
    "\n",
    "        self.local_model.load_state_dict(average_local_weights)\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Returns the weights of the local model in the global network.\n",
    "        \"\"\"\n",
    "        return self.local_model.state_dict()\n",
    "\n",
    "    def play_game(self, max_timesteps=200, num_games=10):\n",
    "        total_reward_sum = 0\n",
    "        for _ in range(num_games):\n",
    "            state = self.test_env.reset()\n",
    "            total_reward = 0\n",
    "            for t in range(max_timesteps):\n",
    "                state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    action_values = self.local_model(state_tensor)\n",
    "                action = torch.argmax(action_values).item()\n",
    "\n",
    "                next_state, reward, done, _ = self.test_env.step(action)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "            total_reward_sum += total_reward\n",
    "\n",
    "        average_reward = total_reward_sum / num_games\n",
    "        return average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f58d371-133b-42db-a406-294ae4387b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This is a distributed Agent class for parallel training in reinforcement learning environments using Ray.\n",
    "    Each Agent has its own environment and own learner for training.\n",
    "    Each Agent instance runs in different nodes and is responsible for collecting experiences\n",
    "    by interacting with its own environment and synchronizing with the global network.\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "TODO: Task 0\n",
    "\"\"\"\n",
    "# Task 0 TODO: add Ray actor decorator here\n",
    "# Hint: You need to specify the number of CPUs for this actor\n",
    "\n",
    "# TODO: Your code here...\n",
    "\n",
    "\"\"\"\n",
    "End of Task 0\n",
    "\"\"\"\n",
    "@ray.remote(num_cpus=2)\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"\n",
    "        Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        self.learner = Learner(state_size, action_size, seed)\n",
    "\n",
    "        # set game environment for training\n",
    "        self.env = gym.make(\"LunarLander-v2\")\n",
    "        self.env.seed(seed)\n",
    "\n",
    "        self.max_t = max_time_steps\n",
    "        self.eps_end = 0.01\n",
    "        self.eps_decay = 0.995\n",
    "        self.eps = 1.0\n",
    "        print(f\"Initializing an Actor on worker with IP: {self.get_worker_ip()}\")\n",
    "\n",
    "    def get_worker_ip(self):\n",
    "        \"\"\"Get the current worker's IP address.\"\"\"\n",
    "        hostname = socket.gethostname()\n",
    "        ip_address = socket.gethostbyname(hostname)\n",
    "        return ip_address\n",
    "\n",
    "    def synchronize_with_global(self, global_net):\n",
    "        \"\"\"\n",
    "        Synchronize the local model weights with the global model by fetching the global model's weights.\n",
    "        Params:\n",
    "            global_net: The handler for the GlobalNet actor.\n",
    "        \"\"\"\n",
    "        global_weights = ray.get(global_net.get_weights.remote())\n",
    "        self.learner.qnetwork_local.load_state_dict(global_weights)\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        \"\"\"\n",
    "        Train the learner's local model in the given environment.\n",
    "        Params\n",
    "        ======\n",
    "            num_episodes (int): Number of episodes to train\n",
    "        \"\"\"\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            score = 0\n",
    "            for t in range(self.max_t):\n",
    "                action = self.learner.act(state, self.eps)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.learner.step(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break\n",
    "            self.eps = max(self.eps_end, self.eps_decay * self.eps)  # decrease epsilon\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Returns the weights of the learner's local models.\n",
    "        \"\"\"\n",
    "        return self.learner.qnetwork_local.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c44671-be64-4484-903a-96fd19bda792",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The driver code to run the distributed training on your Ray cluster. \n",
    "'''\n",
    "\n",
    "# set random seed\n",
    "SEED= 0\n",
    "TEST_SEED = 42\n",
    "random.seed(SEED)\n",
    "# env config\n",
    "state_size = 8\n",
    "action_size = 4\n",
    "\n",
    "\"\"\"\n",
    "TODO: Task 6\n",
    "\"\"\"\n",
    "# hyperparameters you could tune\n",
    "# Task 6 TODO: try different values of the following three hyperparameters once you have finished the other tasks.\n",
    "# Default settings: batch_size = 64, num_epochs_actor_train = 1, num_agents = 4 \n",
    "batch_size = 64  # the minibatch size\n",
    "num_epochs_actor_train = 1  # how many epochs an Agent actor trains before synchronize with GlobalNet\n",
    "num_agents = 4  # how many Agent actors to train in parallel\n",
    "\"\"\"\n",
    "End of Task 6\n",
    "\"\"\"\n",
    "\n",
    "# hyperprameters you don't need to tune\n",
    "buffer_size = int(1e5)  # replay buffer size\n",
    "gamma = 0.99  # discount factor\n",
    "tau = 1e-3  # for soft update of target parameters\n",
    "learning_rate = 5e-4  # learning rate\n",
    "update_every = 4  # how often to update the network\n",
    "max_time_steps = 1000  # max time steps in an episode for train\n",
    "num_episodes = 1000  # how many episodes for training\n",
    "\n",
    "\"\"\"\n",
    "TODO: Task 1\n",
    "\"\"\"\n",
    "# Task 1.1 TODO: Initialize a GlobalNet actor and all N Agent actors where N = num_agents\n",
    "# IMPORTANT: Each agent should take a different random seed so that each covers different random behaviors\n",
    "\n",
    "# TODO: Your code here...\n",
    "# initialize GlobalNet actor\n",
    "global_net = GlobalNet.remote(state_size, action_size, SEED)\n",
    "# initialize Agent actors\n",
    "agent_actors = [ Agent.remote(state_size, action_size, random.ranint(0, 100000)) for _ in range(num_agents) ]\n",
    "\n",
    "time.sleep(1) # This line serves as a barrier. Do not modify this line.\n",
    "\n",
    "\n",
    "# Task 1.2 TODO: Synchronize GlobalNet's model weights to all Agents at the very beginning\n",
    "\n",
    "# TODO: Your code here...\n",
    "# synchronize the initial model weights to all Agent actors\n",
    "ray.get([ aa.synchronize_with_global.remote(global_net) for aa in agent_actors ])\n",
    "\n",
    "\"\"\"\n",
    "End of Task 1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize timers\n",
    "times_update = []\n",
    "times_sync = []\n",
    "times_train = []\n",
    "total_start_time = time.time()\n",
    "\n",
    "average_game_reward = 0\n",
    "for episode in range(num_episodes):\n",
    "    \"\"\"\n",
    "    TODO: Task 2 to Task 5\n",
    "    \"\"\"\n",
    "\n",
    "    # Task 2 TODO: Train all agents in parallel\n",
    "    # Hint: Insert a barrier by calling ray.get(X_future)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # TODO: Your code here... \n",
    "    # explicitly add a barrier (ray.get()) before hitting the next task\n",
    "    ray.get([ aa.train.remote() for aa in agent_actors ])\n",
    "\n",
    "    end_time = time.time()\n",
    "    times_train.append(end_time - start_time)\n",
    "    \n",
    "    # Task 3 TODO: Collect trained weights from each Agent actor and update GlobalNet\n",
    "    start_time = time.time()\n",
    "\n",
    "    # TODO: Your code here... \n",
    "    \n",
    "    end_time = time.time()\n",
    "    times_update.append(end_time - start_time)\n",
    "    \n",
    "    # Task 4 TODO: Each Agent actor performs a model weight synchronization by fetching the global model from GlobalNet\n",
    "    start_time = time.time()\n",
    "\n",
    "    # TODO: Your code here...\n",
    "    \n",
    "    end_time = time.time()\n",
    "    times_sync.append(end_time - start_time)\n",
    "    print(f'Episode {episode} finished.')\n",
    "\n",
    "    # Task 5.1 TODO: Test the global model in GlobalNet and print the score here\n",
    "    \n",
    "    # TODO: Your code here...\n",
    "        \n",
    "    # Task 5.2 and 5.3 TODO: Set the termination condition and save the GlobalNet weights as a checkpoint file\n",
    "    # Hint: Call torch.save(your_model_weights, './model_checkpoint.pth') to save a model checkpoint\n",
    "    \n",
    "    # TODO: Your code here\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    End of Task 2 to Task 5\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "total_end_time = time.time()\n",
    "\n",
    "print(f\"Total duration: {total_end_time - total_start_time:.2f} seconds\")\n",
    "print(f\"Average time for training: {np.mean(times_train):.2f} seconds\")\n",
    "print(f\"Average time for updating: {np.mean(times_update):.2f} seconds\")\n",
    "print(f\"Average time for synchronizing: {np.mean(times_sync):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9468c3c2-f230-4270-a121-fccc61277941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disconnect the ray cluster by ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17ee79-09da-4c58-a6e6-733a70bf0a4e",
   "metadata": {},
   "source": [
    "# Visualize the game playing just for fun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6b52c-d5de-40d4-a352-d08d9c7d8f99",
   "metadata": {},
   "source": [
    "The code snippets below are fully implemented and are optional. \n",
    "It provides a way for you to visualize how your trained model plays Lunar Lander. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73251890-86ef-4789-9b01-1a212d18712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "import socket\n",
    "from pyvirtualdisplay import Display\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e33ac-83f2-48bd-9add-3e8bdb07eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Duplicate classes needed for testing\n",
    "'''\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_unit=64,\n",
    "                 fc2_unit=64):\n",
    "        \"\"\"\n",
    "        Initialize parameters and build model.\n",
    "        Params\n",
    "        =======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_unit (int): Number of nodes in first hidden layer\n",
    "            fc2_unit (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_unit)\n",
    "        self.fc2 = nn.Linear(fc1_unit, fc2_unit)\n",
    "        self.fc3 = nn.Linear(fc2_unit, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Build a network that maps state -> action values.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float()\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float()\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "        \n",
    "'''\n",
    "The Agent is interacting with the environment to learn an optimal policy for decision-making.\n",
    "'''\n",
    "class Agent:\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states\n",
    "        q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update target network\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d718bb5-b675-4d3e-a407-4e446a988710",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the previously saved trained model from the last checkpoint \n",
    "and do tests to display a visualization of the game being played.\n",
    "'''\n",
    "import gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "import random\n",
    "# set random seed\n",
    "SEED=0\n",
    "TEST_SEED=42\n",
    "random.seed(SEED)\n",
    "\n",
    "# hyper parameters\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64  # minibatch size\n",
    "GAMMA = 0.99  # discount factor\n",
    "TAU = 1e-3  # for soft update of target parameters\n",
    "LR = 5e-4  # learning rate\n",
    "UPDATE_EVERY = 4  # how often to update the network\n",
    "MAX_TIME_STEPS= 1000  # max time steps in an episode\n",
    "num_episodes = 1000   # how many episodes for training\n",
    "\n",
    "random.seed(0)\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(42)\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "\n",
    "# TODO: You may need to modify the path accordingly if the default does not match.\n",
    "model_path='./model_checkpoint.pth'\n",
    "agent.qnetwork_local.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "\n",
    "def display_animation(frames):\n",
    "    \"\"\"\n",
    "    Take a list of frames and display them as an animation.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
    "    display(HTML(anim.to_jshtml()))\n",
    "\n",
    "# run several episodes and show games\n",
    "for i in range(5):\n",
    "    state = env.reset()\n",
    "    frames = []\n",
    "    for j in range(1000):    # set max steps to 1000\n",
    "        action = agent.act(state)\n",
    "        frames.append(env.render(mode=\"rgb_array\"))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    display_animation(frames)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
